{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Top'></a>\n",
    "# Test Runner Harness\n",
    "This Repo contains a framework/harness for running tests of various agents against tasks or environments. This file is intended to explain.\n",
    "\n",
    "1. [How to create an agent](#Agent)\n",
    "2. [How to create a task](#Task)\n",
    "3. [How to use the test runner framework](#runner)\n",
    "\n",
    "It also includes a list of [Tricks](#Tricks) and possible [TODOs](#TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='Agent'></a>\n",
    "## 1) How to create an agent\n",
    "[Top](#Top)\n",
    "\n",
    "When we get to the test runner, there are two options for passing an agent. Either passing the agent object directly, or by specifying by name. If you wish to use the specifying by name option you MUST make your agent class a subclass of the BaseAgent class([/agents/base_agent.py](./agents/base_agent.py)) plus some extra steps (see below). If you are passing in an agent object this is not necessary but it can be helpful as it will give helpful errors when you have forgotten to implement something.\n",
    "\n",
    "Regardless of if they are subclassed or not and agent MUST have the following methods:\n",
    "\n",
    "1. `__init__(self, env)`\n",
    "2. `act(self, state, testing)` \n",
    "3. `learn(self, state, action, reward, next_state, done, testing)`\n",
    "4. `reset(self)`\n",
    "\n",
    "### __init__\n",
    "The init function should take an task or environment object as an argument. It is expected that after initialization all the functions will work. Though reset() is called before every \"run\" (not episode) of a test so you can do some building of the network there too if you really wanted.\n",
    "\n",
    "### act\n",
    "The act function takes arguments     \n",
    "state - The state being passed from the environment     \n",
    "testing -  A boolean indicating if this episode is a testing episode    \n",
    "\n",
    "The act function should return a choice of action which is in the environment's action space.\n",
    "\n",
    "### learn\n",
    "The learn function is called after every step in an episode with:    \n",
    "state - the previous state acted on    \n",
    "action - the action which was taken    \n",
    "reward - the reward that was given    \n",
    "next_state - the state which resulted from the action    \n",
    "done - boolean indicating if the episode has finished    \n",
    "testing - boolean indicating if it is currently a testing round    \n",
    "\n",
    "### reset\n",
    "The purpose of reset is to support re-initializing the network so that the test runner can run one agent against one environment multiple times (runs) in sequence in order to accumulate average learning data. Reset should make the agent \"like new\". This can be tricky to do. But there are options which I'll list in the [Tricks](#Tricks) section \n",
    "\n",
    "\n",
    "### Example Agent\n",
    "Here I have created a sample agent which performs the random policy and never learns:\n",
    "\n",
    "```python\n",
    "from agents.base_agent import BaseAgent\n",
    "class RandomAgent(BaseAgent):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def act(self, state, testing):\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def learn(self, s, a, r, sp, d, t):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "```\n",
    "\n",
    "Technically in this case reset() doesn't need to be explicitly defined because the BaseAgent class has a basic implementation which simply passes\n",
    "\n",
    "### Making your agent accessible by name\n",
    "The test runner supports referencing an agent by name and it fetching it for you. In order for it to do this your class **MUST** be a subclass of BaseAgent. It must also be loaded in the kernel at the time of running the test runner. The simplest way to do this if you done fiddling with it is to same it in a file in /agents/ and to import your agent class in the [/agents/\\_\\_init__.py](./agents/__init__.py)\n",
    "\n",
    "You can also reference by name if you have constructed the class in a notebook and it inherited from BaseAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='Task'></a>\n",
    "## How to create and environments/ tasks\n",
    "[Top](#Top)\n",
    "\n",
    "Here we have a similar situation to the agents above. The test runner supports both taking a task/environment object or a name. When using the Name option the same restrictions apply. You must subclass BaseTask ([/tasks/base_task.py](./tasks/base_task.py))\n",
    "\n",
    "Tasks are designed to mimic open ai gym environments so that all open ai gym environments will be automatically compatible with the test runner. Note though that reference by name is not yet supported for open ai gyms  because they are not subclasses of BaseTask if using a gym you MUST pass the object.\n",
    "\n",
    "In order to meet this compatibility requirement the following methods are required for any task:\n",
    "\n",
    "1. `__init__(self)`\n",
    "2. `step(self, action)`\n",
    "3. `reset(self)`\n",
    "4. `render(self, close=False)`\n",
    "\n",
    "### __init__\n",
    "There are actually no special requirements for initialization. Though it is recommended to include observation and action spaces so that agents can infer what input and output shapes they need to satisfy.\n",
    "\n",
    "### step\n",
    "Step takes an action and is expected to output the following as a tuple\n",
    "\n",
    "reward - numerical reward value    \n",
    "next_state - the new state of the environment    \n",
    "done - Boolean indicating if the episode has ended    \n",
    "info - I'm not really sure what is supposed to be in this but gym supports it so so must you.    \n",
    "\n",
    "### reset\n",
    "Reset takes no arguments it just resets to the start of a new episode. It returns the initial state.\n",
    "\n",
    "### render\n",
    "This one is the most complicated. It is only used in the test runner's display functionality which allows you to see a video of the agent working in the environment. It must support the keyword argument close which by default is False and is True when the window is to be closed. This is soley for compatibility with the gyn environments. and is kind of optional to implement.\n",
    "\n",
    "### Example\n",
    "Here is an example where I have wrapped an existing gym environment to reduce the information supplied to it in the state (Removing the velocity data)\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from gym import spaces\n",
    "from base_task import BaseTask\n",
    "\n",
    "class limitedCartPole(BaseTask):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        \n",
    "        self.mask = [0,2]\n",
    "        olow  = self.env.observation_space.low[self.mask]\n",
    "        ohigh = self.env.observation_space.high[self.mask]\n",
    "        \n",
    "        self.observation_space = spaces.Box(olow, ohigh)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "    def step(self, *args, **kwargs):\n",
    "        ns, r, d, i = self.env.step(*args, **kwargs)\n",
    "        ns = ns[self.mask]\n",
    "        return ns, r, d, i\n",
    "    \n",
    "    def reset(self):\n",
    "        s = self.env.reset()\n",
    "        s = s[self.mask]\n",
    "        return s\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.env.render(*args, **kwargs)\n",
    "```\n",
    "\n",
    "\n",
    "### Making your task accessible by name\n",
    "The test runner supports referencing an task by name and it fetching it for you. In order for it to do this your class **MUST** be a subclass of BaseTask. It must also be loaded in the kernel at the time of running the test runner. The simplest way to do this if you done fiddling with it is to same it in a file in /tasks/ and to import your task class in the [/tasks/\\_\\_init__.py](./tasks/__init__.py)\n",
    "\n",
    "You can also reference by name if you have constructed the class in a notebook and it inherited from BaseTask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='Runner'></a>\n",
    "## Using the test runner\n",
    "\n",
    "The test runner is a harness for taking an agent and a task and running them through episodes and recording stats. It is still in its early stages but it is capable of decent configuration of the test as will be outlined here. in particular it is responsible for determining if it is a testing or training round. It also has helper functions for inspecting the results\n",
    "\n",
    "### __init__\n",
    "The parameters to init are listed below\n",
    "`(self, env, agent, runs=1, num_episodes=1000, report_interval=10, test_samples=5, print_test=False)`\n",
    "\n",
    "env - The environment object or string name of task to be used    \n",
    "agent - The agent object or the string name of agent to be used    \n",
    "runs - How many times the agent should be reset and trained to accumulate statisics on learning ability    \n",
    "num_episodes - How many episodes are in a single run    \n",
    "report_interval - How frequently a testing phase should occur and stats be recorded    \n",
    "test_sample - How many episodes should be used as a test sample when testing    \n",
    "print_test - Boolean indicating if only the results of testing phases should be printed out    \n",
    "\n",
    "**NOTE** When using the string name option use the name of the class even if you \"import ... as ...\" python doesn't care what you call it when its looking for the class name\n",
    "\n",
    "### start\n",
    "Dtart takes no arguments. It will reset the testrunner and start a testing sequence\n",
    "\n",
    "### display\n",
    "Display takes no arguments. It runs the agent through 1 episode while calling env.render() to display the agent at work\n",
    "\n",
    "### plot\n",
    "Plot takes one keyword argument, mode, which if set to 'ci' will plot the mean of all runs along with the 95% confidence interval. Otherwise it will plot all runs on one graph.\n",
    "\n",
    "### Example\n",
    "Simple examples of using the testRunner are as follows:\n",
    "\n",
    "```python\n",
    "from runner import TestRunner\n",
    "\n",
    "runner = TestRunner('limitedCartPole', 'RandomAgent', 100, report_interval=5, print_test=True, runs=10)\n",
    "\n",
    "# Run the test\n",
    "runner.start()\n",
    "\n",
    "# display the agent at work\n",
    "runner.display()\n",
    "\n",
    "# show the performance plots\n",
    "runner.plot(mode='ci')\n",
    "```\n",
    "\n",
    "Alternatively both env and agent can be actual objects. This is particularly useful for supplying open AI gyms\n",
    "\n",
    "```python\n",
    "from runner import TestRunner\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "agent = RandomAgent(env)\n",
    "\n",
    "runner = TestRunner(env, agent, 100, report_interval=5, print_test=True, runs=10)\n",
    "\n",
    "...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='Tricks'></a>\n",
    "## Tricks\n",
    "[Top](#Top)\n",
    "\n",
    "Here are a few \"tricks\" that might come in handy\n",
    "\n",
    "### Saving a rendering to file\n",
    "Open AI gym has a method for doing this, though in my experience it is very finicky. In involves wrapping the environment.\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "Menv = wrappers.Monitor(env, './videos/Cartpole-agent')\n",
    "```\n",
    "\n",
    "You can then either pass this wrapped environment to the test runner directly OR if you can swap it in after the fact (if maybe you forgot but you got a really good agent and want to record it) by editing the `runner.env` and replacing it with the wrapped version. You can then call display() and it will play a single episode and save the video in the folder provided.\n",
    "\n",
    "```python\n",
    "# Replace the runner's env temporarily\n",
    "# Menv.reset()\n",
    "runner.env = Menv\n",
    "runner.display()\n",
    "#This is a dirty hack to make it save\n",
    "Menv.reset()\n",
    "Menv.render(close=True)\n",
    "\n",
    "# Switch back\n",
    "runner.env = env\n",
    "```\n",
    "\n",
    "Note there is some weirdness about when it chooses to save. If you are getting videos that are empty then try doing a `Menv.reset()` either before or after calling display or both... Not really sure why that happens\n",
    "\n",
    "### Accessing the rewards data\n",
    "Sometimes you might want to get the reward data  after having run the test runner. All of the reward information is stored in `runner.tracker` which is a numpy array of shape `[num_reports, num_runs]`. If you also want the episode numbers which correspond to these data points, access `runner.X` which is of shape `[num_reports]`. num_reports is the number of times testing phase was run. This is approximately `num_episodes//reporting_interval` but might be 1 more because testing is always run on the last episode of a run.\n",
    "\n",
    "### Accessing the Agent\n",
    "If you want to retrieve you agent after the fact it is stored at `runner.agent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id='TODO'></a>\n",
    "## TODO\n",
    "[Top](#Top)\n",
    "\n",
    "This is a work in progress harness which will probably get adapted as it is used. Some things foreseen to include:\n",
    "\n",
    "1. Agent checkpointing options, either best or every test etc. This will require exposing a .save() function on the agent\n",
    "2. The ability to input hyper parameters and possibly even sets of hyper parameter options have them run all of them and store the test results. This is similar to something that sci-kit learn has for grid searching\n",
    "3. Maybe more built in analysis options than just plot. Other stats\n",
    "4. The ability to retrieve processed stats like the mean and confidence intervales so multiple tests can be compared on one plot easier\n",
    "5. MAYBE allow for multiple agents to be specified all of which will be run and stats accumulated. this could be added in conjunction with 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RL]",
   "language": "python",
   "name": "conda-env-RL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
